{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "# Import Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert module root directory to sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath(os.path.join(os.path.abspath(\"\"), '..'))\n",
    "sys.path.insert(0, ROOT_DIR)\n",
    "from run import MountainCar_v0\n",
    "from src import ReplayBuffer\n",
    "from src.Utils import get_logger, eval_dict_values, parameter_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is trained with different combinations of the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DDQNS = [True]\n",
    "EPS_DECAYS = np.linspace(0.1, 0.5, num=2, dtype=np.float16)\n",
    "EXPLORE_EXPLOIT_INTERVALS = np.linspace(10, 40, num=1, dtype=np.uint16)\n",
    "TARGET_UPDATE_STEPS = [10]#np.linspace(5, 20, num=4, dtype=np.uint16)\n",
    "EXPLORE_RATIOS = [0.25]#np.linspace(0.1, 0.5, num=3, dtype=np.float16)\"\"\"\n",
    "SEEDS = np.random.randint(100, 1000, size=1, dtype=np.uint16)\n",
    "LOG_INIT_KWARGS = {\"eps_decay\": np.linspace(0.1, 0.5, num=2, dtype=np.float16), \n",
    "                   \"ddqn\": [True]}\n",
    "LOG_TRAIN_KWARGS = {\"target_update_steps\": [10],\n",
    "                    \"explore_exploit_interval\": np.linspace(10, 40, num=1, dtype=np.uint16),\n",
    "                    \"explore_ratio\": [0.25]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"log\"\n",
    "SUMM_DIR = \"summaries\"\n",
    "CONFIG_DIR = \"config\"\n",
    "ENV_NAME = \"MountainCar-v0\"\n",
    "TF_CONFIG = tf_v1.ConfigProto(gpu_options=tf_v1.GPUOptions(per_process_gpu_memory_fraction=0.5), \n",
    "                              allow_soft_placement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result = False                        # Plots the result in matplotlib\n",
    "test_model_chkpt = None                    # Address to a trained model checkpoint\n",
    "record_interval = 0\n",
    "epochs = 1000\n",
    "date_time = datetime.now().strftime(\"%d.%m.%Y %H.%M\")\n",
    "# Root directory needed to be specified explicitly in Jupyter Notebook\n",
    "summ_dir = os.path.join(ROOT_DIR, SUMM_DIR, \"{} {}\".format(ENV_NAME, date_time))\n",
    "log_file = os.path.join(summ_dir, LOG_DIR, \"Results {} {}.log\".format(ENV_NAME, date_time))\n",
    "config_file = os.path.join(ROOT_DIR, CONFIG_DIR, \"MountainCar-v0.ini\")\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load configuration from .ini file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = MountainCar_v0.get_configuration(config_file)\n",
    "init_kwargs, train_kwargs = config_dict[\"kwargs\"]\n",
    "mem_size = config_dict[\"others\"][\"mem_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train agent with different parameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0918 20:34:26.090476  6096 deprecation.py:506] From c:\\users\\raj k\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0918 20:34:26.159483  6096 deprecation.py:323] From c:\\users\\raj k\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Training: Model 1\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 100, mean_loss: 2.2852, total_reward: -131.0, max_pos: 0.5131, in 83.0421 secs\n",
      "Epoch: 200, mean_loss: 1.0264, total_reward: -141.0, max_pos: 0.5161, in 67.2969 secs\n",
      "Epoch: 300, mean_loss: 0.5350, total_reward: -105.0, max_pos: 0.5041, in 63.1733 secs\n",
      "Epoch: 400, mean_loss: 0.6702, total_reward: -108.0, max_pos: 0.5199, in 57.2305 secs\n",
      "Epoch: 500, mean_loss: 0.0757, total_reward: -92.0, max_pos: 0.5048, in 56.5376 secs\n",
      "Epoch: 600, mean_loss: 0.0733, total_reward: -85.0, max_pos: 0.5183, in 55.4536 secs\n",
      "Epoch: 700, mean_loss: 0.0672, total_reward: -85.0, max_pos: 0.5061, in 61.8077 secs\n",
      "Epoch: 800, mean_loss: 0.0756, total_reward: -91.0, max_pos: 0.5141, in 62.9434 secs\n",
      "Epoch: 900, mean_loss: 0.0868, total_reward: -89.0, max_pos: 0.5030, in 63.6945 secs\n",
      "Epoch: 1000, mean_loss: 0.1804, total_reward: -87.0, max_pos: 0.5105, in 55.2383 secs\n",
      "                                                  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Goals achieved: 297                 \n",
      "First goal achieved: -109.98 mean reward at 361 epoch.\n",
      "Max goal achieved: -102.52 mean reward at 1000 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Training: Model 2\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 100, mean_loss: 3.0779, total_reward: -200.0, max_pos: -0.3309, in 103.0931 secs\n",
      "Epoch: 200, mean_loss: 2.1854, total_reward: -175.0, max_pos: 0.5369, in 85.0755 secs\n",
      "Epoch: 300, mean_loss: 1.1283, total_reward: -103.0, max_pos: 0.5197, in 74.9167 secs\n",
      "Epoch: 400, mean_loss: 0.8847, total_reward: -119.0, max_pos: 0.5224, in 65.6842 secs\n",
      "Epoch: 500, mean_loss: 0.5534, total_reward: -89.0, max_pos: 0.5104, in 59.7909 secs\n",
      "Epoch: 600, mean_loss: 0.1124, total_reward: -85.0, max_pos: 0.5190, in 55.0037 secs\n",
      "Epoch: 700, mean_loss: 0.1832, total_reward: -85.0, max_pos: 0.5052, in 65.0482 secs\n",
      "Epoch: 800, mean_loss: 0.2949, total_reward: -143.0, max_pos: 0.5070, in 69.8976 secs\n",
      "Epoch: 900, mean_loss: 0.2101, total_reward: -87.0, max_pos: 0.5019, in 63.7277 secs\n",
      "Training agent. Please be patient...\r"
     ]
    }
   ],
   "source": [
    "# Setup logger\n",
    "logger = get_logger(log_file)\n",
    "# Create environment and replay buffer\n",
    "env = gym.make(ENV_NAME)\n",
    "mem = ReplayBuffer(mem_size)\n",
    "if record_interval > 0:\n",
    "    # Wrap environment with Monitor wrapper to record videos\n",
    "    env = gym.wrappers.Monitor(env, os.path.join(summ_dir, \"videos\"), force=True,\n",
    "                               video_callable=lambda epoch: not epoch%record_interval)\n",
    "para_gen = parameter_generator(SEEDS, LOG_INIT_KWARGS, LOG_TRAIN_KWARGS)\n",
    "for model_i, (seed, log_init_kwargs, log_train_kwargs) in enumerate(para_gen, start=1):\n",
    "    # Run the program\n",
    "    MountainCar_v0.run(env, seed, mem, logger, summ_dir, epochs, init_kwargs, train_kwargs, log_init_kwargs, \n",
    "                       log_train_kwargs, plot_result, model_i=model_i, sess_config=TF_CONFIG, \n",
    "                       test_model_chkpt=test_model_chkpt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
