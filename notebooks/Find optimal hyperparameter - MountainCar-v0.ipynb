{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "# Import Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert module root directory to sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath(os.path.join(os.path.abspath(\"\"), '..'))\n",
    "sys.path.insert(0, ROOT_DIR)\n",
    "from run import MountainCar_v0\n",
    "from src import ReplayBuffer\n",
    "from src.Utils import get_logger, eval_dict_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates **init_kwargs** and **train_kwargs**. *init_kwargs* contains keyworded-arguments to pass while initializing the agent whereas *train_kwargs* contains keyworded-arguments to pass while training the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def parameter_generator_old(eps_decays, exp_intervals, target_update_steps, explore_ratios, ddqns, seeds):\n",
    "    for eps_decay in eps_decays:\n",
    "        for exp_interval in exp_intervals:\n",
    "            for target_update_step in target_update_steps:\n",
    "                for exp_ratio in explore_ratios:\n",
    "                    for ddqn in ddqns:\n",
    "                        for seed in seeds:\n",
    "                            init_kwargs = {\"eps_decay\": eps_decay, \"explore_exploit_interval\":exp_interval, \"DDQN\":ddqn}\n",
    "                            train_kwargs = {\"target_update_steps\":target_update_step, \"explore_ratio\":exp_ratio}\n",
    "                            yield seed, init_kwargs, train_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def parameter_generator(eps_decays, exp_intervals, target_update_steps, explore_ratios, ddqns, seeds):\n",
    "    init_parameters = product(eps_decays, exp_intervals, ddqns)\n",
    "    train_parameters = product(target_update_steps, explore_ratios)\n",
    "    for para in product(init_parameters, train_parameters, seeds):\n",
    "        init_args, train_args, seed = para\n",
    "        init_kwargs = {\"eps_decay\": init_args[0], \"explore_exploit_interval\":init_args[1], \"DDQN\":init_args[2]}\n",
    "        train_kwargs = {\"target_update_steps\":train_args[0], \"explore_ratio\":train_args[1]}\n",
    "        yield seed, init_kwargs, train_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function receives parameters like tensorflow **sess**, OpenAI gym **env**, model id and keyworded arguments to initialize and train the agent.  \n",
    "\n",
    "The function performs following tasks:\n",
    "- Creates a *ReplayBuffer* to store transitions **(state, action, reward, next_state, done)** of the agent.\n",
    "- Creates an *agent*\n",
    "- Creates tf.train.Saver object to save/restore agent model (as checkpoints)\n",
    "- Trains the agent for given number of episodes, measures goal achievements, logs the summaries of each epoch (average loss, epoch length, maximum position, total epoch reward) in Tensorboard and returns the final summaries\n",
    "- Saves trained model\n",
    "- Plots the summaries using matplotlib if **plot_result** is set to True.\n",
    "- Returns only the goal summary (number of achieved goals, first achieved goal, goal with most reward) where the format of each goal is *(epoch and epoch reward)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Agent init:\n",
    "1. Fixed parameters: sess, env, eps, lr, df, tau, render, (change later) [mem, batch_size]\n",
    "2. Variable parameters: eps_decay, explore_exploit_interval, DDQN\n",
    "\n",
    "Agent train:\n",
    "1. Fixed parameters: NUM_EPISODES, display_every, goal_trials, goal_reward\n",
    "2. Variable parameters: target_update_steps, explore_ratio\n",
    "\"\"\"\n",
    "def run(sess, env, model_dir, summ_dir, model_i, init_kwargs, train_kwargs, plot_result=False):\n",
    "    model_name = \"Model {}\".format(model_i)\n",
    "    print(\"# Training: {}\".format(model_name))\n",
    "    summ_dir = os.path.join(summ_dir, model_name)\n",
    "    # create replay buffer, agent and (model checkpoint) saver\n",
    "    mem = ReplayBuffer(160_000)\n",
    "    agent = MountainCar(sess, env, mem, batch_size=100, eps=(1, 0.001), lr=0.97, \n",
    "                           df=0.99, tau=1, render=render, summ_dir=summ_dir, \n",
    "                           **init_kwargs)\n",
    "    # TODO: Increase this size\n",
    "    saver = tf.train.Saver(max_to_keep=100)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    results= agent.train(NUM_EPISODES, display_every=DISPLAY_RATE,\n",
    "                         goal_trials=100, goal_reward=-110.0,\n",
    "                         **train_kwargs)\n",
    "    saver.save(sess, os.path.join(model_dir, model_name))\n",
    "    *results, goal_summary = results\n",
    "    if plot_result:\n",
    "        for p, plt_name in zip(results, (\"Losses\", \"Rewards\", \"Max pos\", \"Epoch length\")):\n",
    "            plt.plot(p)\n",
    "            plt.xlabel('Episodes')\n",
    "            plt.ylabel(plt_name)\n",
    "            plt.show()\n",
    "            plt.close(\"all\")\n",
    "    return goal_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore logs\n",
    "**8th Sept 2019 - 22:53** \n",
    "- eps_decay:(0.1, 0.5, 3), \n",
    "- explore_interval:(5, 20, 4), \n",
    "- target_update_steps:(5, 20, 4), \n",
    "- explore_ratios:(0.1, 0.5, 3), \n",
    "- random seeds:(100, 1000, 2)\n",
    "\n",
    "**12th Sept 2019 - 23:47** \n",
    "- eps_decay:(0.1, 0.5, 5), \n",
    "- explore_interval:(10, 40, 4), \n",
    "- target_update_steps:(10, 1), \n",
    "- explore_ratios:(0.25, 1), \n",
    "- random seeds:(100, 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"log\"\n",
    "SUMM_DIR = \"summaries\"\n",
    "CONFIG_DIR = \"config\"\n",
    "ENV_NAME = \"MountainCar-v0\"\n",
    "SEEDS = np.random.randint(100, 1000, size=1, dtype=np.uint16)\n",
    "TF_CONFIG = tf_v1.ConfigProto(gpu_options=tf_v1.GPUOptions(per_process_gpu_memory_fraction=0.5), \n",
    "                              allow_soft_placement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Default summary directoy, log and config file\n",
    "    date_time = datetime.now().strftime(\"%d.%m.%Y %H.%M\")\n",
    "    summ_dir = os.path.join(SUMM_DIR, \"{} {}\".format(ENV_NAME, date_time))\n",
    "    log_file = os.path.join(summ_dir, LOG_DIR, \"Results {} {}.log\".format(ENV_NAME, date_time))\n",
    "    config_file = os.path.join(CONFIG_DIR, \"MountainCar-v0.ini\")\n",
    "    # Parse configuration from config file\n",
    "    config_parser = configparser.ConfigParser()\n",
    "    \"\"\"\n",
    "    Somehow in Jupyter Notebook config_parser keeps looking in ROOT_DIR\\notebooks\\config_dir \n",
    "    instead of ROOT_DIR\\config_dir, even though the ROOT_DIR is in sys.path. \n",
    "    Therefore, explicit addressing is required in Jupyter Notebook.\n",
    "    \"\"\"\n",
    "    config_parser.read(os.path.join(ROOT_DIR, config_file))\n",
    "    # Load configurations from config file\n",
    "    init_kwargs = eval_dict_values(config_parser[\"init_kwargs\"])\n",
    "    train_kwargs = eval_dict_values(config_parser[\"train_kwargs\"])\n",
    "    mem_size = eval(config_parser[\"others\"].get(\"mem_size\", 50_000))\n",
    "    # Setup logger\n",
    "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "    logger = get_logger(log_file)\n",
    "    # Create environment and replay buffer\n",
    "    env = gym.make(ENV_NAME)\n",
    "    mem = ReplayBuffer(mem_size)\n",
    "    if record_interval > 0:\n",
    "        # Wrap environment with Monitor wrapper to record videos\n",
    "        env = gym.wrappers.Monitor(env, os.path.join(summ_dir, \"videos\"), force=True,\n",
    "                                   video_callable=lambda epoch: not epoch%record_interval)\n",
    "    # Testing model\n",
    "    if test_model_chkpt is not None:\n",
    "        # Override goal_trials and display_every parameters\n",
    "        train_kwargs[\"goal_trials\"] = 1\n",
    "        train_kwargs[\"display_every\"] = train_kwargs[\"epochs\"]/10\n",
    "    for init_kwargs, train_kwargs in \n",
    "    # Run the program\n",
    "    MountainCar_v0.run(env, SEEDS, mem, logger, summ_dir, init_kwargs, train_kwargs, \n",
    "                       log_init_kwargs, log_train_kwargs, plot_result, sess_config=TF_CONFIG, \n",
    "                       test_model_chkpt=test_model_chkpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is trained with different combinations of the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDQNS = [True]\n",
    "EPS_DECAYS = np.linspace(0.1, 0.5, num=5, dtype=np.float16)\n",
    "EXPLORE_EXPLOIT_INTERVALS = np.linspace(10, 40, num=4, dtype=np.uint16)\n",
    "TARGET_UPDATE_STEPS = [10]#np.linspace(5, 20, num=4, dtype=np.uint16)\n",
    "EXPLORE_RATIOS = [0.25]#np.linspace(0.1, 0.5, num=3, dtype=np.float16)\n",
    "SEEDS = np.random.randint(100, 1000, size=2, dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_generator(eps_decays, exp_intervals, target_update_steps, explore_ratios, ddqns):\n",
    "    init_parameters = product(eps_decays, ddqns)\n",
    "    train_parameters = product(target_update_steps, exp_intervals, explore_ratios)\n",
    "    for param in product(init_parameters, train_parameters):\n",
    "        init_args, train_args= param\n",
    "        init_kwargs = {\"eps_decay\": init_args[0], \n",
    "                       \"ddqn\":init_args[1]}\n",
    "        train_kwargs = {\"target_update_steps\":train_args[0], \n",
    "                        \"explore_exploit_interval\":init_args[1], \n",
    "                        \"explore_ratio\":train_args[2]}\n",
    "        yield init_kwargs, train_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eps_decay': 0.1, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.1, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.1, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.1, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.2, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.2, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.2, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.2, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.3, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.3, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.3, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.3, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.4, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.4, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.4, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.4, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.5, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.5, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.5, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n",
      "{'eps_decay': 0.5, 'ddqn': True} {'target_update_steps': 10, 'explore_exploit_interval': True, 'explore_ratio': 0.25}\n"
     ]
    }
   ],
   "source": [
    "for k1, k2 in parameter_generator(EPS_DECAYS, EXPLORE_EXPLOIT_INTERVALS, TARGET_UPDATE_STEPS, EXPLORE_RATIOS, DDQNS):\n",
    "    print(k1, k2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore plan\n",
    "1. Set parameters as far apart as possible.\n",
    "2. Determine parameter ranges that produce reasonably good results\n",
    "3. Set parameters within this ranges or explore new parameter range\n",
    "4. Repeat step 2  \n",
    "\n",
    "# Further plan\n",
    "- Create class to explore hyperparameters. \n",
    "- The class includes parameter_generator and run methods.\n",
    "- It would probably have a context manager option to save and load trained parameters information\n",
    "- Store and plot achieved goal information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparameter_Checker:\n",
    "    def __init__(self, env, mem, model_dir, summ_dir, log_file, start_index=1, saver_max_size=100, \n",
    "                 batch_size=100, display_every=100, epochs=1000, render=False):\n",
    "        self.env = env\n",
    "        self.mem = mem\n",
    "        self.saver_max_size = saver_max_size\n",
    "        self.display_every = display_every\n",
    "        self.start_index = start_index\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.render = render\n",
    "        self.model_dir = model_dir\n",
    "        self.summ_dir = summ_dir\n",
    "        self.log_file = log_file\n",
    "        self.logger = get_logger(self.log_file)\n",
    "        \n",
    "    @staticmethod\n",
    "    def parameter_generator(eps_decays, exp_intervals, target_update_steps, explore_ratios, ddqns, seeds):\n",
    "        init_parameters = product(eps_decays, exp_intervals, ddqns)\n",
    "        train_parameters = product(target_update_steps, explore_ratios)\n",
    "        for param in product(init_parameters, train_parameters, seeds):\n",
    "            init_args, train_args, seed = param\n",
    "            init_kwargs = {\"eps_decay\": init_args[0], \"explore_exploit_interval\":init_args[1], \"ddqn\":init_args[2]}\n",
    "            train_kwargs = {\"target_update_steps\":train_args[0], \"explore_ratio\":train_args[1]}\n",
    "            yield seed, init_kwargs, train_kwargs\n",
    "    \n",
    "    def run(self, sess, model_i, init_kwargs, train_kwargs, seed=None, plot_result=False):\n",
    "        model_name = \"Model {}\".format(model_i)\n",
    "        print(\"# Training: {}\".format(model_name))\n",
    "        summ_dir = os.path.join(self.summ_dir, model_name)\n",
    "        agent = MountainCar(sess, self.env, self.mem, batch_size=self.batch_size, eps_limits=(1, 0.001), \n",
    "                               lr=0.97, df=0.99, tau=1, render=self.render, summ_dir=summ_dir, \n",
    "                               **init_kwargs)\n",
    "        saver = tf.train.Saver(max_to_keep=self.saver_max_size)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        results= agent.train(self.epochs, display_every=self.display_every, \n",
    "                             goal_trials=100, goal_reward=-110.0, \n",
    "                             **train_kwargs)\n",
    "        saver.save(sess, os.path.join(self.model_dir, model_name))\n",
    "        *results, goal_summary = results\n",
    "        if plot_result:\n",
    "            for p, plt_name in zip(results, (\"Losses\", \"Rewards\", \"Max pos\", \"Epoch length\")):\n",
    "                plt.plot(p)\n",
    "                plt.xlabel('Episodes')\n",
    "                plt.ylabel(plt_name)\n",
    "                plt.show()\n",
    "                plt.close(\"all\")\n",
    "        parameter_dict = {\"seed\":seed, **init_kwargs, **train_kwargs}\n",
    "        agent.log(self.logger, model_i, parameter_dict, goal_summary)\n",
    "    \n",
    "    def check_parameters(self, *args, plot_result=False):\n",
    "        para_gen = self.parameter_generator(*args)\n",
    "        for model_i, (seed, init_kwargs, train_kwargs) in enumerate(para_gen, start=1):\n",
    "            if model_i < self.start_index:\n",
    "                continue                        # Skip parameters until the model_i >= start_index \n",
    "            self.env.seed(int(seed))\n",
    "            with tf.Session(config=TF_CONFIG) as sess:\n",
    "                self.run(sess, model_i, seed=seed, init_kwargs=init_kwargs,\n",
    "                         train_kwargs=train_kwargs, plot_result=plot_result)\n",
    "            tf.compat.v1.reset_default_graph()\n",
    "            self.mem.clear()\n",
    "             \"\"\"\n",
    "            parameter_str = dict2str(parameter_dict)\n",
    "            self.logger.debug(\"Model {:<2} - {}\".format(model_i, parameter_str))\n",
    "            num_goals, first_goal, max_goal = goal_summary\n",
    "            first_goal_epoch, first_goal_reward = first_goal \n",
    "            max_goal_epoch, max_goal_reward = max_goal\n",
    "            self.logger.debug(\"Goals achieved: {}\".format(num_goals))\n",
    "            if num_goals:\n",
    "                self.logger.info(\"First goal achieved: {:.2f} mean reward at {} epoch.\".format(first_goal_reward, first_goal_epoch))\n",
    "            self.logger.info(\"Max goal achieved: {:.2f} mean reward at {} epoch.\\n\".format(max_goal_reward, max_goal_epoch))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"Log\"\n",
    "ENV_NAME = \"MountainCar-v0\"\n",
    "DISPLAY_RATE = 250\n",
    "NUM_EPISODES = 1000\n",
    "render = False\n",
    "TF_CONFIG = tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.25), \n",
    "                           allow_soft_placement=True)\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.mkdir(LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following tasks are performed in the block below:\n",
    "- Generate different combinations of parameters\n",
    "- Initialize the agent with parameters in **init_kwargs**\n",
    "- Train the agent with parameters in **train_kwargs**\n",
    "- Reset tensorflow graph (to reuse the graph to train future models)\n",
    "- Log goal summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0913 00:06:05.869809  8472 deprecation.py:506] From c:\\users\\raj k\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0913 00:06:05.979811  8472 deprecation.py:323] From c:\\users\\raj k\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 1\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 2.9343, total_reward: -143.0, max_pos: 0.5059,  in 266.3583 secs\n",
      "Epoch: 500, mean_loss: 0.8678, total_reward: -103.0, max_pos: 0.5135,  in 184.2229 secs\n",
      "Epoch: 750, mean_loss: 0.1018, total_reward: -200.0, max_pos: -0.0846,  in 159.7277 secs\n",
      "Epoch: 1000, mean_loss: 0.2040, total_reward: -97.0, max_pos: 0.5052,  in 155.1219 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.99 mean reward at 703 epoch.\n",
      "Max goal achieved: -105.18 mean reward at 890 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 2\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.5008, total_reward: -174.0, max_pos: 0.5044,  in 213.1094 secs\n",
      "Epoch: 500, mean_loss: 0.1313, total_reward: -116.0, max_pos: 0.5105,  in 168.3576 secs\n",
      "Epoch: 750, mean_loss: 0.1695, total_reward: -171.0, max_pos: 0.5053,  in 163.2149 secs\n",
      "Epoch: 1000, mean_loss: 0.1052, total_reward: -140.0, max_pos: 0.5369,  in 158.5637 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.88 mean reward at 486 epoch.\n",
      "Max goal achieved: -105.55 mean reward at 556 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 3\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.4848, total_reward: -150.0, max_pos: 0.5369,  in 215.4989 secs\n",
      "Epoch: 500, mean_loss: 0.4096, total_reward: -98.0, max_pos: 0.5168,  in 182.8998 secs\n",
      "Epoch: 750, mean_loss: 0.1913, total_reward: -111.0, max_pos: 0.5251,  in 166.8978 secs\n",
      "Epoch: 1000, mean_loss: 0.2037, total_reward: -104.0, max_pos: 0.5201,  in 157.7732 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.71 mean reward at 908 epoch.\n",
      "Max goal achieved: -106.78 mean reward at 918 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 4\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 3.2807, total_reward: -200.0, max_pos: 0.2555,  in 270.0422 secs\n",
      "Epoch: 500, mean_loss: 0.5327, total_reward: -160.0, max_pos: 0.5369,  in 189.8401 secs\n",
      "Epoch: 750, mean_loss: 0.0716, total_reward: -106.0, max_pos: 0.5073,  in 165.2046 secs\n",
      "Epoch: 1000, mean_loss: 0.0712, total_reward: -121.0, max_pos: 0.5109,  in 153.0079 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.47 mean reward at 691 epoch.\n",
      "Max goal achieved: -103.39 mean reward at 778 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 5\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 0.5877, total_reward: -116.0, max_pos: 0.5241,  in 200.4850 secs\n",
      "Epoch: 500, mean_loss: 0.1541, total_reward: -92.0, max_pos: 0.5115,  in 154.5413 secs\n",
      "Epoch: 750, mean_loss: 0.1527, total_reward: -103.0, max_pos: 0.5069,  in 154.7825 secs\n",
      "Epoch: 1000, mean_loss: 0.1285, total_reward: -195.0, max_pos: 0.5369,  in 156.8226 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.99 mean reward at 440 epoch.\n",
      "Max goal achieved: -102.40 mean reward at 696 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 6\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.4924, total_reward: -176.0, max_pos: 0.5214,  in 230.0499 secs\n",
      "Epoch: 500, mean_loss: 0.1782, total_reward: -115.0, max_pos: 0.5454,  in 187.6773 secs\n",
      "Epoch: 750, mean_loss: 0.0849, total_reward: -112.0, max_pos: 0.5268,  in 165.8811 secs\n",
      "Epoch: 1000, mean_loss: 0.0916, total_reward: -116.0, max_pos: 0.5168,  in 157.3814 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.86 mean reward at 544 epoch.\n",
      "Max goal achieved: -103.87 mean reward at 926 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 7\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 2.2192, total_reward: -149.0, max_pos: 0.5407,  in 223.2527 secs\n",
      "Epoch: 500, mean_loss: 0.1585, total_reward: -87.0, max_pos: 0.5045,  in 164.2207 secs\n",
      "Epoch: 750, mean_loss: 0.0476, total_reward: -103.0, max_pos: 0.5000,  in 146.5226 secs\n",
      "Epoch: 1000, mean_loss: 0.0486, total_reward: -96.0, max_pos: 0.5133,  in 153.2426 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.72 mean reward at 491 epoch.\n",
      "Max goal achieved: -100.69 mean reward at 686 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 8\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.5654, total_reward: -169.0, max_pos: 0.5408,  in 234.7432 secs\n",
      "Epoch: 500, mean_loss: 0.1637, total_reward: -105.0, max_pos: 0.5196,  in 168.6585 secs\n",
      "Epoch: 750, mean_loss: 0.0591, total_reward: -103.0, max_pos: 0.5121,  in 147.9427 secs\n",
      "Epoch: 1000, mean_loss: 0.2665, total_reward: -107.0, max_pos: 0.5139,  in 168.5768 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.98 mean reward at 487 epoch.\n",
      "Max goal achieved: -103.02 mean reward at 740 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 9\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 2.0339, total_reward: -128.0, max_pos: 0.5366,  in 234.0919 secs\n",
      "Epoch: 500, mean_loss: 0.1968, total_reward: -106.0, max_pos: 0.5113,  in 176.2980 secs\n",
      "Epoch: 750, mean_loss: 0.1848, total_reward: -157.0, max_pos: 0.5058,  in 169.4055 secs\n",
      "Epoch: 1000, mean_loss: 0.3459, total_reward: -121.0, max_pos: 0.5117,  in 180.3913 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Max goal achieved: -112.45 mean reward at 993 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 10\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.0492, total_reward: -106.0, max_pos: 0.5109,  in 200.4614 secs\n",
      "Epoch: 500, mean_loss: 0.1386, total_reward: -110.0, max_pos: 0.5383,  in 163.3435 secs\n",
      "Epoch: 750, mean_loss: 0.2454, total_reward: -109.0, max_pos: 0.5275,  in 156.7363 secs\n",
      "Epoch: 1000, mean_loss: 0.1114, total_reward: -106.0, max_pos: 0.5186,  in 166.6664 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.92 mean reward at 494 epoch.\n",
      "Max goal achieved: -101.88 mean reward at 662 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 11\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 0.7892, total_reward: -114.0, max_pos: 0.5144,  in 192.4873 secs\n",
      "Epoch: 500, mean_loss: 0.0716, total_reward: -90.0, max_pos: 0.5061,  in 154.9714 secs\n",
      "Epoch: 750, mean_loss: 0.0675, total_reward: -137.0, max_pos: 0.5053,  in 161.7462 secs\n",
      "Epoch: 1000, mean_loss: 0.1319, total_reward: -94.0, max_pos: 0.5006,  in 157.7278 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.84 mean reward at 453 epoch.\n",
      "Max goal achieved: -102.72 mean reward at 538 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 12\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.0747, total_reward: -105.0, max_pos: 0.5075,  in 210.3209 secs\n",
      "Epoch: 500, mean_loss: 0.1347, total_reward: -107.0, max_pos: 0.5281,  in 162.1171 secs\n",
      "Epoch: 750, mean_loss: 0.0576, total_reward: -103.0, max_pos: 0.5255,  in 147.5844 secs\n",
      "Epoch: 1000, mean_loss: 0.6476, total_reward: -117.0, max_pos: 0.5275,  in 195.4626 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.85 mean reward at 457 epoch.\n",
      "Max goal achieved: -100.81 mean reward at 641 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 13\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 2.1481, total_reward: -110.0, max_pos: 0.5246,  in 235.7889 secs\n",
      "Epoch: 500, mean_loss: 0.2566, total_reward: -88.0, max_pos: 0.5078,  in 175.0849 secs\n",
      "Epoch: 750, mean_loss: 0.1032, total_reward: -106.0, max_pos: 0.5166,  in 169.8058 secs\n",
      "Epoch: 1000, mean_loss: 0.1056, total_reward: -108.0, max_pos: 0.5251,  in 156.6637 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.68 mean reward at 828 epoch.\n",
      "Max goal achieved: -105.23 mean reward at 922 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 14\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.2501, total_reward: -111.0, max_pos: 0.5012,  in 209.8535 secs\n",
      "Epoch: 500, mean_loss: 0.1115, total_reward: -146.0, max_pos: 0.5092,  in 168.2120 secs\n",
      "Epoch: 750, mean_loss: 0.2076, total_reward: -122.0, max_pos: 0.5339,  in 172.9032 secs\n",
      "Epoch: 1000, mean_loss: 0.2238, total_reward: -114.0, max_pos: 0.5046,  in 183.3610 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Max goal achieved: -110.37 mean reward at 477 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 15\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 2.6961, total_reward: -110.0, max_pos: 0.5398,  in 230.5918 secs\n",
      "Epoch: 500, mean_loss: 0.1538, total_reward: -104.0, max_pos: 0.5080,  in 173.7329 secs\n",
      "Epoch: 750, mean_loss: 0.1374, total_reward: -119.0, max_pos: 0.5135,  in 159.9844 secs\n",
      "Epoch: 1000, mean_loss: 0.2237, total_reward: -121.0, max_pos: 0.5007,  in 168.6535 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.47 mean reward at 561 epoch.\n",
      "Max goal achieved: -107.73 mean reward at 565 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 16\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.7333, total_reward: -138.0, max_pos: 0.5058,  in 219.7741 secs\n",
      "Epoch: 500, mean_loss: 0.1340, total_reward: -150.0, max_pos: 0.5133,  in 198.3640 secs\n",
      "Epoch: 750, mean_loss: 0.1831, total_reward: -106.0, max_pos: 0.5064,  in 177.4350 secs\n",
      "Epoch: 1000, mean_loss: 0.1559, total_reward: -112.0, max_pos: 0.5367,  in 166.5221 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.84 mean reward at 827 epoch.\n",
      "Max goal achieved: -109.35 mean reward at 834 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 17\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.6865, total_reward: -173.0, max_pos: 0.5152,  in 233.8512 secs\n",
      "Epoch: 500, mean_loss: 0.1890, total_reward: -87.0, max_pos: 0.5113,  in 177.8812 secs\n",
      "Epoch: 750, mean_loss: 0.4426, total_reward: -154.0, max_pos: 0.5257,  in 169.4072 secs\n",
      "Epoch: 1000, mean_loss: 0.4804, total_reward: -100.0, max_pos: 0.5076,  in 167.5443 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Max goal achieved: -110.01 mean reward at 705 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 18\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 2.2643, total_reward: -176.0, max_pos: 0.5263,  in 229.7552 secs\n",
      "Epoch: 500, mean_loss: 0.2520, total_reward: -119.0, max_pos: 0.5231,  in 165.0447 secs\n",
      "Epoch: 750, mean_loss: 0.0940, total_reward: -117.0, max_pos: 0.5141,  in 168.8520 secs\n",
      "Epoch: 1000, mean_loss: 0.7671, total_reward: -162.0, max_pos: 0.5244,  in 189.3947 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.71 mean reward at 508 epoch.\n",
      "Max goal achieved: -107.19 mean reward at 581 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 19\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.0889, total_reward: -200.0, max_pos: 0.4243,  in 222.6695 secs\n",
      "Epoch: 500, mean_loss: 0.1186, total_reward: -133.0, max_pos: 0.5243,  in 177.7401 secs\n",
      "Epoch: 750, mean_loss: 0.2015, total_reward: -106.0, max_pos: 0.5291,  in 159.1947 secs\n",
      "Epoch: 1000, mean_loss: 0.1583, total_reward: -95.0, max_pos: 0.5147,  in 169.4934 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.76 mean reward at 539 epoch.\n",
      "Max goal achieved: -101.00 mean reward at 844 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 20\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.7926, total_reward: -137.0, max_pos: 0.5151,  in 220.9041 secs\n",
      "Epoch: 500, mean_loss: 0.1246, total_reward: -105.0, max_pos: 0.5022,  in 170.5050 secs\n",
      "Epoch: 750, mean_loss: 0.1329, total_reward: -105.0, max_pos: 0.5145,  in 165.8094 secs\n",
      "Epoch: 1000, mean_loss: 0.2084, total_reward: -148.0, max_pos: 0.5369,  in 189.8364 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.69 mean reward at 518 epoch.\n",
      "Max goal achieved: -105.44 mean reward at 558 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 21\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.7768, total_reward: -154.0, max_pos: 0.5268,  in 235.0237 secs\n",
      "Epoch: 500, mean_loss: 0.1768, total_reward: -101.0, max_pos: 0.5146,  in 175.5240 secs\n",
      "Epoch: 750, mean_loss: 0.1073, total_reward: -104.0, max_pos: 0.5052,  in 148.3885 secs\n",
      "Epoch: 1000, mean_loss: 0.2064, total_reward: -100.0, max_pos: 0.5096,  in 179.0574 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.82 mean reward at 525 epoch.\n",
      "Max goal achieved: -100.46 mean reward at 697 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 22\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 0.5865, total_reward: -113.0, max_pos: 0.5050,  in 209.7948 secs\n",
      "Epoch: 500, mean_loss: 0.0752, total_reward: -105.0, max_pos: 0.5103,  in 156.4617 secs\n",
      "Epoch: 750, mean_loss: 0.0561, total_reward: -105.0, max_pos: 0.5027,  in 154.7993 secs\n",
      "Epoch: 1000, mean_loss: 0.1047, total_reward: -108.0, max_pos: 0.5182,  in 163.7310 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.91 mean reward at 436 epoch.\n",
      "Max goal achieved: -99.88 mean reward at 558 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 23\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 4.5058, total_reward: -200.0, max_pos: -0.4063,  in 277.0203 secs\n",
      "Epoch: 500, mean_loss: 2.4840, total_reward: -87.0, max_pos: 0.5045,  in 240.2121 secs\n",
      "Epoch: 750, mean_loss: 0.1131, total_reward: -148.0, max_pos: 0.5359,  in 192.4407 secs\n",
      "Epoch: 1000, mean_loss: 0.1289, total_reward: -103.0, max_pos: 0.5078,  in 162.4567 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.91 mean reward at 970 epoch.\n",
      "Max goal achieved: -103.75 mean reward at 1000 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 24\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.9288, total_reward: -161.0, max_pos: 0.5177,  in 236.4940 secs\n",
      "Epoch: 500, mean_loss: 0.1143, total_reward: -105.0, max_pos: 0.5015,  in 183.2397 secs\n",
      "Epoch: 750, mean_loss: 0.8264, total_reward: -164.0, max_pos: 0.5369,  in 198.8283 secs\n",
      "Epoch: 1000, mean_loss: 0.2676, total_reward: -112.0, max_pos: 0.5167,  in 160.3707 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.63 mean reward at 956 epoch.\n",
      "Max goal achieved: -104.43 mean reward at 993 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 25\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 2.4363, total_reward: -200.0, max_pos: 0.3399,  in 238.4883 secs\n",
      "Epoch: 500, mean_loss: 0.1119, total_reward: -87.0, max_pos: 0.5045,  in 170.2142 secs\n",
      "Epoch: 750, mean_loss: 0.2350, total_reward: -200.0, max_pos: -0.1106,  in 163.4048 secs\n",
      "Epoch: 1000, mean_loss: 0.2373, total_reward: -111.0, max_pos: 0.5128,  in 165.9842 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.14 mean reward at 494 epoch.\n",
      "Max goal achieved: -105.43 mean reward at 577 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 26\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 3.6653, total_reward: -160.0, max_pos: 0.5284,  in 275.4822 secs\n",
      "Epoch: 500, mean_loss: 0.6212, total_reward: -144.0, max_pos: 0.5382,  in 190.9317 secs\n",
      "Epoch: 750, mean_loss: 0.0951, total_reward: -109.0, max_pos: 0.5083,  in 175.0992 secs\n",
      "Epoch: 1000, mean_loss: 0.1217, total_reward: -110.0, max_pos: 0.5175,  in 169.9652 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Max goal achieved: -111.79 mean reward at 818 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 27\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.8631, total_reward: -155.0, max_pos: 0.5197,  in 242.0336 secs\n",
      "Epoch: 500, mean_loss: 0.3668, total_reward: -91.0, max_pos: 0.5001,  in 176.9696 secs\n",
      "Epoch: 750, mean_loss: 0.2484, total_reward: -200.0, max_pos: -0.4794,  in 165.8898 secs\n",
      "Epoch: 1000, mean_loss: 0.3413, total_reward: -100.0, max_pos: 0.5100,  in 174.1226 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.84 mean reward at 541 epoch.\n",
      "Max goal achieved: -102.75 mean reward at 581 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 28\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.5805, total_reward: -103.0, max_pos: 0.5217,  in 235.2102 secs\n",
      "Epoch: 500, mean_loss: 0.2090, total_reward: -117.0, max_pos: 0.5048,  in 176.4830 secs\n",
      "Epoch: 750, mean_loss: 0.0585, total_reward: -103.0, max_pos: 0.5046,  in 163.2884 secs\n",
      "Epoch: 1000, mean_loss: 0.1564, total_reward: -111.0, max_pos: 0.5155,  in 179.0756 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.87 mean reward at 637 epoch.\n",
      "Max goal achieved: -106.07 mean reward at 676 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 29\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 0.9672, total_reward: -109.0, max_pos: 0.5251,  in 201.1118 secs\n",
      "Epoch: 500, mean_loss: 0.0707, total_reward: -87.0, max_pos: 0.5045,  in 157.5159 secs\n",
      "Epoch: 750, mean_loss: 0.7580, total_reward: -159.0, max_pos: 0.5093,  in 185.8383 secs\n",
      "Epoch: 1000, mean_loss: 0.3524, total_reward: -130.0, max_pos: 0.5038,  in 176.1020 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.66 mean reward at 460 epoch.\n",
      "Max goal achieved: -103.33 mean reward at 556 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 30\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.8634, total_reward: -149.0, max_pos: 0.5429,  in 233.1951 secs\n",
      "Epoch: 500, mean_loss: 0.2617, total_reward: -111.0, max_pos: 0.5269,  in 178.6010 secs\n",
      "Epoch: 750, mean_loss: 0.1179, total_reward: -106.0, max_pos: 0.5075,  in 159.9608 secs\n",
      "Epoch: 1000, mean_loss: 0.0905, total_reward: -123.0, max_pos: 0.5057,  in 178.7412 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.74 mean reward at 560 epoch.\n",
      "Max goal achieved: -105.73 mean reward at 593 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 31\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 2.2923, total_reward: -109.0, max_pos: 0.5092,  in 247.7342 secs\n",
      "Epoch: 500, mean_loss: 0.2375, total_reward: -98.0, max_pos: 0.5160,  in 184.4436 secs\n",
      "Epoch: 750, mean_loss: 0.1104, total_reward: -110.0, max_pos: 0.5089,  in 165.3132 secs\n",
      "Epoch: 1000, mean_loss: 0.1552, total_reward: -94.0, max_pos: 0.5006,  in 178.4806 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -110.00 mean reward at 753 epoch.\n",
      "Max goal achieved: -107.77 mean reward at 801 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 32\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.9353, total_reward: -108.0, max_pos: 0.5093,  in 241.7991 secs\n",
      "Epoch: 500, mean_loss: 0.5116, total_reward: -116.0, max_pos: 0.5361,  in 169.3740 secs\n",
      "Epoch: 750, mean_loss: 0.0746, total_reward: -155.0, max_pos: 0.5178,  in 151.7585 secs\n",
      "Epoch: 1000, mean_loss: 0.0756, total_reward: -155.0, max_pos: 0.5302,  in 163.5846 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.82 mean reward at 623 epoch.\n",
      "Max goal achieved: -103.37 mean reward at 703 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 33\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.8478, total_reward: -133.0, max_pos: 0.5349,  in 235.5952 secs\n",
      "Epoch: 500, mean_loss: 0.4156, total_reward: -96.0, max_pos: 0.5207,  in 165.8315 secs\n",
      "Epoch: 750, mean_loss: 0.0925, total_reward: -104.0, max_pos: 0.5151,  in 155.4280 secs\n",
      "Epoch: 1000, mean_loss: 0.1992, total_reward: -100.0, max_pos: 0.5082,  in 156.7521 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.97 mean reward at 537 epoch.\n",
      "Max goal achieved: -104.62 mean reward at 811 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 34\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.9249, total_reward: -172.0, max_pos: 0.5259,  in 242.0428 secs\n",
      "Epoch: 500, mean_loss: 0.2335, total_reward: -115.0, max_pos: 0.5196,  in 180.6443 secs\n",
      "Epoch: 750, mean_loss: 0.2346, total_reward: -104.0, max_pos: 0.5182,  in 156.3839 secs\n",
      "Epoch: 1000, mean_loss: 0.1198, total_reward: -111.0, max_pos: 0.5129,  in 166.7506 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.95 mean reward at 559 epoch.\n",
      "Max goal achieved: -105.26 mean reward at 588 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 35\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 3.6521, total_reward: -200.0, max_pos: -0.0525,  in 273.1309 secs\n",
      "Epoch: 500, mean_loss: 1.5010, total_reward: -175.0, max_pos: 0.5369,  in 204.9385 secs\n",
      "Epoch: 750, mean_loss: 0.1823, total_reward: -123.0, max_pos: 0.5386,  in 169.3977 secs\n",
      "Epoch: 1000, mean_loss: 0.1580, total_reward: -135.0, max_pos: 0.5058,  in 176.1023 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Max goal achieved: -112.61 mean reward at 714 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 36\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.6841, total_reward: -155.0, max_pos: 0.5397,  in 223.1463 secs\n",
      "Epoch: 500, mean_loss: 0.0980, total_reward: -114.0, max_pos: 0.5049,  in 170.5037 secs\n",
      "Epoch: 750, mean_loss: 0.0846, total_reward: -106.0, max_pos: 0.5180,  in 164.3030 secs\n",
      "Epoch: 1000, mean_loss: 0.0851, total_reward: -108.0, max_pos: 0.5157,  in 148.1344 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.48 mean reward at 524 epoch.\n",
      "Max goal achieved: -102.06 mean reward at 931 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 37\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 4.5886, total_reward: -200.0, max_pos: -0.2997,  in 274.3156 secs\n",
      "Epoch: 500, mean_loss: 1.3963, total_reward: -99.0, max_pos: 0.5140,  in 221.3381 secs\n",
      "Epoch: 750, mean_loss: 0.1167, total_reward: -104.0, max_pos: 0.5012,  in 180.9376 secs\n",
      "Epoch: 1000, mean_loss: 0.3341, total_reward: -111.0, max_pos: 0.5218,  in 189.4740 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Max goal achieved: -119.60 mean reward at 858 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 38\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 2.7057, total_reward: -189.0, max_pos: 0.5211,  in 243.2824 secs\n",
      "Epoch: 500, mean_loss: 0.1961, total_reward: -112.0, max_pos: 0.5003,  in 173.3239 secs\n",
      "Epoch: 750, mean_loss: 0.0547, total_reward: -108.0, max_pos: 0.5149,  in 148.4334 secs\n",
      "Epoch: 1000, mean_loss: 0.7353, total_reward: -122.0, max_pos: 0.5088,  in 178.0942 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.82 mean reward at 516 epoch.\n",
      "Max goal achieved: -102.39 mean reward at 648 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 39\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 2.6428, total_reward: -151.0, max_pos: 0.5369,  in 244.2484 secs\n",
      "Epoch: 500, mean_loss: 0.3937, total_reward: -91.0, max_pos: 0.5164,  in 185.7556 secs\n",
      "Epoch: 750, mean_loss: 0.5960, total_reward: -104.0, max_pos: 0.5117,  in 191.7711 secs\n",
      "Epoch: 1000, mean_loss: 0.6848, total_reward: -108.0, max_pos: 0.5162,  in 165.2275 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.89 mean reward at 883 epoch.\n",
      "Max goal achieved: -106.78 mean reward at 918 epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: Model 40\n",
      "Goal: Get average reward of -110.00 over 100 consecutive trials!\n",
      "Epoch: 250, mean_loss: 1.0653, total_reward: -197.0, max_pos: 0.5369,  in 246.8239 secs\n",
      "Epoch: 500, mean_loss: 0.2191, total_reward: -126.0, max_pos: 0.5440,  in 180.1326 secs\n",
      "Epoch: 750, mean_loss: 0.1431, total_reward: -153.0, max_pos: 0.5086,  in 183.2748 secs\n",
      "Epoch: 1000, mean_loss: 0.3423, total_reward: -142.0, max_pos: 0.5015,  in 202.7662 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First goal achieved: -109.53 mean reward at 442 epoch.\n",
      "Max goal achieved: -102.61 mean reward at 486 epoch.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(ENV_NAME)\n",
    "    mem = ReplayBuffer(50_000)\n",
    "    \"\"\"model_dir = r'models\\Model 08.09.2019 23.05'\n",
    "    summ_dir = r'summaries\\MountainCar-v0 08.09.2019 23.05'\n",
    "    log_file = r'Log\\Results 08.09.2019 23.05.txt'\"\"\"\n",
    "    date_time = datetime.now().strftime(\"%d.%m.%Y %H.%M\")\n",
    "    model_dir = os.path.join(\"models\", \"Model {}\".format(date_time))\n",
    "    summ_dir = os.path.join(\"summaries\", \"{} {}\".format(ENV_NAME, date_time))\n",
    "    log_file = os.path.join(LOG_DIR, \"Results {}.txt\".format(date_time))\n",
    "    checker = Hyperparameter_Checker(env, mem, model_dir, summ_dir, log_file, start_index=1, \n",
    "                                     saver_max_size=300, display_every=DISPLAY_RATE, epochs=NUM_EPISODES,\n",
    "                                     render=render)\n",
    "    checker.check_parameters(EPS_DECAYS, EXPLORE_EXPLOIT_INTERVALS, TARGET_UPDATE_STEPS, \n",
    "                             EXPLORE_RATIOS, DDQNS, SEEDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(ENV_NAME)\n",
    "    date_time = datetime.now().strftime(\"%d.%m.%Y %H.%M\")\n",
    "    model_dir = os.path.join(\"models\", \"Model {}\".format(date_time))\n",
    "    summ_dir = os.path.join(\"summaries\", \"{} {}\".format(ENV_NAME, date_time))\n",
    "    log_file = os.path.join(LOG_DIR, \"Results {}.txt\".format(date_time))\n",
    "    logger = get_logger(log_file)\n",
    "    para_gen = parameter_generator(EPS_DECAYS, EXPLORE_EXPLOIT_INTERVALS, TARGET_UPDATE_STEPS, \n",
    "                                   EXPLORE_RATIOS, DDQNS, SEEDS)\n",
    "    for model_i, (seed, init_kwargs, train_kwargs) in enumerate(para_gen, start=1):\n",
    "        env.seed(int(seed))\n",
    "        with tf.Session(config=TF_CONFIG) as sess:\n",
    "            goal_summary = run(sess, env, model_dir, summ_dir, model_i, init_kwargs=init_kwargs,\n",
    "                               train_kwargs=train_kwargs, plot_result=False)\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        # Log results\n",
    "        parameter_dict = {\"seed\":seed, **init_kwargs, **train_kwargs} \n",
    "        parameter_str = dict2str(parameter_dict)\n",
    "        logger.debug(\"Model {:<2} - {}\".format(model_i, parameter_str))\n",
    "        num_goals, first_goal, max_goal = goal_summary\n",
    "        first_goal_epoch, first_goal_reward = first_goal \n",
    "        max_goal_epoch, max_goal_reward = max_goal\n",
    "        logger.debug(\"Goals achieved: {}\".format(num_goals))\n",
    "        if num_goals:\n",
    "            logger.info(\"First goal achieved: {:.2f} mean reward at {} epoch.\".format(first_goal_reward, first_goal_epoch))\n",
    "        logger.info(\"Max goal achieved: {:.2f} mean reward at {} epoch.\\n\".format(max_goal_reward, max_goal_epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
