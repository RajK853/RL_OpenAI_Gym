HumanoidDeepMimicWalkBulletEnv: &base_config
  env_name: HumanoidDeepMimicWalkBulletEnv-v1
  epochs: 2000
  record_interval: 10
  render: False
  summary_dir: summaries/pybullet
  include: [pybullet_envs]
  algo: 
    name: sac
    kwargs: &algo_configs
      target_entropy: -15.0
      num_gradient_steps: 1000
      num_init_exp_samples: 10000
      max_init_exp_timestep: auto
      tau: 0.01
      update_interval: 10
      batch_size_kwargs:
        type: ConstantScheduler
        value: 32
      gamma_kwargs:
        type: ConstantScheduler
        value: 0.997
      q_lr_kwargs:
        type: ConstantScheduler
        value: 0.0003
      alpha_lr_kwargs:
        type: ConstantScheduler
        value: 0.0003
      layers:
        - {type: Dense, units: 256, kernel_regularizer: l2}
        - {type: LayerNormalization}
        - {type: Activation, activation: relu}
        - {type: Dense, units: 256, kernel_regularizer: l2}
        - {type: LayerNormalization}
        - {type: Activation, activation: relu}
        - {type: Dense, units: 100, kernel_regularizer: l2}
        - {type: LayerNormalization}
        - {type: Activation, activation: relu}
        - {type: Dense, units: 1}
  policy:
    name: gaussian
    kwargs: &policy_configs
      mu_range: [-2.0, 2.0]
      log_std_range: [-20.0, 0.3]
      lr_kwargs:
        type: ConstantScheduler
        value: 0.0003
      layers:
        - {type: Dense, units: 256, kernel_regularizer: l2}
        - {type: LayerNormalization}
        - {type: Activation, activation: relu}
        - {type: Dense, units: 256, kernel_regularizer: l2}
        - {type: LayerNormalization}
        - {type: Activation, activation: relu}
        - {type: Dense, units: 100, kernel_regularizer: l2}
        - {type: LayerNormalization}
        - {type: Activation, activation: relu}